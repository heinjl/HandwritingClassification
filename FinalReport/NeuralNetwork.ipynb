{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model Results\n",
    "\n",
    "My best neural network model acheived 1.58% test error. Here are the details about how the training works with this model.\n",
    "\n",
    "784-1024-1024-1024-10 Logistic-Softmax Model\n",
    "* Minibatch Stoichastic Training - The training dataset is split into batches of \n",
    "100 and the neural network is trained on one batch at a time. This speeds up the training significantly.\n",
    "* Learning rate - 0.28%\n",
    "* Momentum - 95% of the change in synapse from a previous step is added to the change for the next step. This adds another hyperparameter to be tuned\n",
    "* Max-Norm Regularization - With a softmax activation on the output, it is possible for the synapse values to become exceedingly high and cause inf values to appear as output. To prevent this from being an issue, the magnitude of any output node's synpase is restricted to 10. This should not affect the actual output since softmax by nature regularizes the output.\n",
    "* Dropout - A method to prevent overfitting on the training set in theory by turning off input and hidden nodes randomly during forward propogation and backpropogation. In the training of this network, 50% of hidden nodes are dropped at a time and 20% of input pixels are also dropped.\n",
    "\n",
    "The following link is the classifier object in the final application, where a neural network model can be trained/used.\n",
    "Link to [Classifier](classifier.py)\n",
    "\n",
    "The rest of the notebook is the testing of a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "filepath = \"ClassifierData/\"\n",
    "\n",
    "# softmax function\n",
    "def softmax(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1\n",
    "    exp_scores = np.exp(x)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# sigmoid/logistic function\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Retrieve the test data from the filesystem\n",
    "data = pd.read_csv(\"Kaggle Competition MINST train.csv\")\n",
    "target = data['label']\n",
    "data = data.drop('label', axis=1)\n",
    "data = data.div(255)\n",
    "\n",
    "# Split the training data so that I can analyze testing error (same split as training)\n",
    "train_data, test_data, train_target, test_target = cross_validation.train_test_split(\n",
    " data, target, test_size=0.25, random_state=0)\n",
    "num_attributes = len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural network architecture parameters\n",
    "hidden_layer_funct = sigmoid\n",
    "output_layer_funct = softmax\n",
    "\n",
    "input_layer_size = num_attributes\n",
    "num_hidden_layers = 3\n",
    "hidden_layer_size = 1024\n",
    "output_layer_size = 10\n",
    "\n",
    "# Save the synapses for later use\n",
    "synapse = []\n",
    "biases = []\n",
    "for index in range(num_hidden_layers + 1):\n",
    "    syn_filename = filepath + \"%d-Layer %s-%s %d-%d-%d nodes syn%d.csv\" % (\n",
    "       num_hidden_layers, hidden_layer_funct.func_name, output_layer_funct.func_name, input_layer_size, hidden_layer_size, output_layer_size, index)\n",
    "    bias_filename = filepath + \"%d-Layer %s-%s %d-%d-%d nodes bias%d.csv\" % (\n",
    "       num_hidden_layers, hidden_layer_funct.func_name, output_layer_funct.func_name, input_layer_size, hidden_layer_size, output_layer_size, index)\n",
    "    synapse.append(np.array(pd.read_csv(syn_filename).drop(\"Unnamed: 0\", axis = 1)))\n",
    "    biases.append(np.array(pd.read_csv(bias_filename).drop(\"Unnamed: 0\", axis = 1)))\n",
    "\n",
    "def pred(input):\n",
    "    current_layer = input\n",
    "    for layer in range(num_hidden_layers):\n",
    "        current_layer = hidden_layer_funct(np.dot(current_layer, synapse[layer])\n",
    "                                            + biases[layer].T)\n",
    "        \n",
    "    output = output_layer_funct(np.dot(current_layer, synapse[num_hidden_layers]) + biases[num_hidden_layers].T)\n",
    "    \n",
    "    answer = np.zeros(len(output), dtype = np.int8)\n",
    "    for i in range(len(output)):\n",
    "        answer[i] = output[i].argmax()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:  0.0\n",
      "Testing Error:  0.0158095238095\n"
     ]
    }
   ],
   "source": [
    "# Test the model at this iteration\n",
    "prediction = pred(train_data)\n",
    "train_error = 1 - accuracy_score(train_target, prediction)\n",
    "prediction = pred(test_data)\n",
    "test_error = 1 - accuracy_score(test_target, prediction)\n",
    "\n",
    "print \"Training Error: \", train_error\n",
    "print \"Testing Error: \" , test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources of Inspiration\n",
    "\n",
    "* https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf - Tuning hyperparameters and structure of the final network, dropout tips\n",
    "* https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts - 10 nodes as output rather than 1\n",
    "* http://iamtrask.github.io/2015/07/28/dropout/ - Implementing dropout to NN\n",
    "* http://iamtrask.github.io/2015/07/12/basic-python-network/ - Original struture of the network\n",
    "* https://en.wikipedia.org/ - Regarding sigmoid/softmax/tanh/rectifier functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
